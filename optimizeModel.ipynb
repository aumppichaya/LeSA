{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (1.20.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\picha\\appdata\\roaming\\python\\python311\\site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime) (1.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime) (24.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime) (5.29.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\picha\\appdata\\roaming\\python\\python311\\site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Requirement already satisfied: onnxruntime-tools in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: onnx in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime-tools) (1.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime-tools) (1.26.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime-tools) (15.0.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime-tools) (6.1.1)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime-tools) (9.0.0)\n",
      "Requirement already satisfied: py3nvml in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime-tools) (0.2.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnxruntime-tools) (24.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from coloredlogs->onnxruntime-tools) (10.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from onnx->onnxruntime-tools) (5.29.2)\n",
      "Requirement already satisfied: xmltodict in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from py3nvml->onnxruntime-tools) (0.14.2)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\picha\\.conda\\envs\\lesa\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime-tools) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install onnxruntime\n",
    "!pip3 install onnxruntime-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized model saved to models/optimized/LeET.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference\n",
    "import onnx\n",
    "\n",
    "# Load your ONNX model\n",
    "model_path = \"models/LeET.onnx\"\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Perform shape inference\n",
    "inferred_model = SymbolicShapeInference.infer_shapes(model)\n",
    "\n",
    "# Save the optimized model\n",
    "optimized_path = \"models/optimized/LeET.onnx\"\n",
    "onnx.save(inferred_model, optimized_path)\n",
    "print(f\"Optimized model saved to {optimized_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to models/quantization/LeDro.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Paths to input and output models\n",
    "model_input = \"models/optimized/LeDro.onnx\"\n",
    "model_output = \"models/quantization/LeDro.onnx\"\n",
    "\n",
    "# Perform dynamic quantization\n",
    "quantize_dynamic(\n",
    "    model_input,  # Input model path\n",
    "    model_output, # Output model path\n",
    "    weight_type=QuantType.QInt8,  # Quantization type (INT8)\n",
    "    per_channel=True  # Enable per-channel quantization\n",
    ")\n",
    "\n",
    "print(f\"Quantized model saved to {model_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "model = onnx.load(\"models/quantization/LeDro.onnx\")\n",
    "print(model.graph.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 8400)\n",
      "[[[8.48427773e+00 1.59541931e+01 2.13065720e+01 ... 4.65950684e+02\n",
      "   4.87500702e+02 5.25638306e+02]\n",
      "  [1.02783031e+01 9.02434444e+00 7.92623711e+00 ... 6.10531067e+02\n",
      "   6.09212036e+02 6.05621948e+02]\n",
      "  [2.63193398e+01 4.52514191e+01 5.49507599e+01 ... 3.54533936e+02\n",
      "   3.14717834e+02 2.34029022e+02]\n",
      "  [3.25009041e+01 1.91723118e+01 1.58667030e+01 ... 5.94387207e+01\n",
      "   6.18578491e+01 6.83438721e+01]\n",
      "  [2.98023224e-08 2.98023224e-08 1.19209290e-07 ... 1.25175714e-03\n",
      "   7.32481480e-04 1.05947256e-04]\n",
      "  [8.94069672e-08 8.94069672e-08 5.96046448e-08 ... 1.16261840e-03\n",
      "   5.92976809e-04 1.31905079e-04]]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "session = ort.InferenceSession(\"models/optimized/LeDro.onnx\")\n",
    "\n",
    "# สร้าง dummy input tensor ขนาด [1, 3, 640, 640]\n",
    "dummy_input = np.random.rand(1, 3, 640, 640).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {\"images\": dummy_input})\n",
    "\n",
    "# ดูผลลัพธ์\n",
    "print(outputs[0].shape)  # Output shape ควรเป็น [1, 6, N]\n",
    "print(outputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
